{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training and generating rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":16:8\"\n",
    "\n",
    "from dataloader import RTDataLoader\n",
    "from framework import RTFramework\n",
    "from rule_miner import RuleMiner\n",
    "\n",
    "# Paths\n",
    "here = \".\"\n",
    "data_dir = os.path.join(here, \"../datasets/\")\n",
    "\n",
    "dataset = \"family\"\n",
    "\n",
    "dataset_dir = os.path.join(data_dir, dataset)\n",
    "train_file = os.path.join(dataset_dir, \"train.json\")\n",
    "valid_file = os.path.join(dataset_dir, \"valid.txt\")\n",
    "test_file = os.path.join(dataset_dir, \"test.txt\")\n",
    "entities_file = os.path.join(dataset_dir, \"entities.txt\")\n",
    "relations_file = os.path.join(dataset_dir, \"relations.txt\")\n",
    "all_file = os.path.join(dataset_dir, \"all.txt\")\n",
    "\"\"\"Saved paths\"\"\"\n",
    "experiment_dir = os.path.join(here, \"../saved\", dataset)\n",
    "# Model checkpoint for continuing training.\n",
    "checkpoint_dir = os.path.join(experiment_dir, \"checkpoint/\")\n",
    "# Directory to save trained model.\n",
    "model_save_dir = os.path.join(experiment_dir, \"model/\")\n",
    "# Options file.\n",
    "option_file = os.path.join(experiment_dir, \"option.txt\")\n",
    "# Model prediction file.\n",
    "prediction_file = os.path.join(experiment_dir, \"prediction.txt\")\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.mkdir(experiment_dir)\n",
    "    os.mkdir(checkpoint_dir)\n",
    "    os.mkdir(model_save_dir)\n",
    "\"\"\"Other configurations\"\"\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hypterparameters\"\"\"\n",
    "top_k = 10\n",
    "rank = 3\n",
    "num_steps = 2\n",
    "query_embed_dim = 128\n",
    "num_rnn_layers = 1\n",
    "rnn_hidden_size = 128\n",
    "seed = 210224\n",
    "batch_size = 128\n",
    "train_epochs = 20\n",
    "beta_l = 1.\n",
    "beta_s = 0.8\n",
    "lr = 0.001\n",
    "num_sample_batches = 0\n",
    "\n",
    "# Specify random seed.\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = RTDataLoader(\n",
    "    relations_file, entities_file,\n",
    "    all_file, train_file,\n",
    "    valid_file, test_file\n",
    ")\n",
    "\n",
    "num_relations = dataloader.num_relations\n",
    "num_operators = dataloader.num_operators\n",
    "num_entities = dataloader.num_entities\n",
    "\n",
    "dataloader.id2rel[num_operators] = \"self\"\n",
    "dataloader.rel2id[\"self\"] = num_operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "miner = RuleMiner(\n",
    "    rank, num_steps, num_entities,\n",
    "    num_operators, num_operators,\n",
    "    query_embed_dim, num_rnn_layers,\n",
    "    rnn_hidden_size\n",
    ").to(device)\n",
    "optimizer = optim.Adam(miner.parameters(), lr=lr)\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "framework = RTFramework(\n",
    "    miner, optimizer, dataloader,\n",
    "    loss_fn, device, ckpt_save_dir=checkpoint_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framework.train(top_k, batch_size, num_sample_batches, train_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = os.path.join(checkpoint_dir, \"checkpoint.pth.tar\")\n",
    "checkpoint = torch.load(ckpt_file)\n",
    "miner.load_state_dict(checkpoint['model'])\n",
    "framework.eval(\"test\", batch_size, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qq, hh, TT, mat in dataloader.one_epoch(\"test\", 10, shuffle=True):\n",
    "    break\n",
    "qq = torch.from_numpy(qq).to(device)\n",
    "hh = torch.from_numpy(hh).to(device)\n",
    "TT = torch.from_numpy(TT).to(device)\n",
    "logits = miner(qq, hh, TT, mat)\n",
    "print([dataloader.id2rel[rel.item()] for rel in qq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relation = 4\n",
    "print(miner.attention_operator_list[0][:, :, relation, -1].size())\n",
    "print(miner.attention_operator_list[0][:, :, relation, -1])\n",
    "print(dataloader.id2rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "attn_combs = [list(range(num_operators+1)) for _ in range(num_steps)]\n",
    "attn_combs = itertools.product(*attn_combs)\n",
    "path_rank = []\n",
    "for comb in attn_combs:\n",
    "    cur_path = [[], 0.]\n",
    "    step2rel = list(zip(range(len(comb)), comb))\n",
    "    for r in range(rank):\n",
    "        attention_operators = miner.attention_operator_list[r][:, :, relation, -1]\n",
    "        tmp_score = 1.\n",
    "        for step, rel in step2rel:\n",
    "            if r == 0:\n",
    "                cur_path[0].append(dataloader.id2rel[rel])\n",
    "            tmp_score *= attention_operators[step, rel].item()\n",
    "        cur_path[1] += tmp_score\n",
    "    path_rank.append(cur_path)\n",
    "path_rank.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rank[:10]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d3fc4a18242776d773fa4c68ccdfd6100512bee141f96bd1429afa49ad52813"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
